{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24cf7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import matplotlib.patches as mpatches\n",
    "import kagglehub\n",
    "import glob\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c630e7",
   "metadata": {},
   "source": [
    "- T·∫°o class DoSDDoSClassifier v·ªõi ƒë·∫ßy ƒë·ªß ph∆∞∆°ng ph√°p:\n",
    "- Tri·ªÉn khai 4 thu·∫≠t to√°n ML (KNN, AdaBoost, Random Forest, SVM)\n",
    "- Tri·ªÉn khai 3 k·ªπ thu·∫≠t gi·∫£m chi·ªÅu d·ªØ li·ªáu (PCA, Feature Importance, Univariate Selection)\n",
    "- Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (l√†m s·∫°ch, chu·∫©n h√≥a MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6636a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoSDDoSClassifier:\n",
    "    \"\"\"\n",
    "    - Thu·∫≠t to√°n 4 ml: KNN, AdaBoost, Random Forest, SVM\n",
    "    - 3 K·ªπ thu·∫≠t gi·∫£m chi·ªÅu d·ªØ li·ªáu: PCA, Feature Importance, Univariate Selection\n",
    "    - Ho√†n th√†nh ti·ªÅn x·ª≠ l√Ω\n",
    "    - ƒê√°nh gi√° v·ªõi th·ªùi gian v√† ƒë·ªô ch√≠nh x√°c\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Thu·∫≠t to√°n ML\n",
    "        self.ml_algorithms = {\n",
    "            'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "            'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'SVM': SVC(kernel='rbf', random_state=42)\n",
    "        }\n",
    "        \n",
    "        # Gi·∫£m chi·ªÅu d·ªØ li·ªáu\n",
    "        self.feature_selection_methods = {\n",
    "            'PCA': None,  # S·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o ƒë·ªông\n",
    "            'Feature_Importance': None,  # ExtraTree + SelectFromModel\n",
    "            'Univariate_Selection': None  # SelectKBest v·ªõi chi2\n",
    "        }\n",
    "        \n",
    "        # Scaler normalisation MinMax (-1, 1)\n",
    "        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # L∆∞u tr·ªØ k·∫øt qu·∫£ v√† d·ªØ li·ªáu ƒë√£ gi·∫£m chi·ªÅu\n",
    "        self.results = {}\n",
    "        self.feature_reduced_data = {}\n",
    "        \n",
    "    def preprocess_data(self, X, y, dataset_name=\"Dataset\"):\n",
    "        \"\"\"\n",
    "        Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:\n",
    "        1. L√†m s·∫°ch gi√° tr·ªã NaN v√† v√¥ c√πng\n",
    "        2. Lo·∫°i b·ªè c√°c features b·∫±ng 0\n",
    "        3. Chu·∫©n h√≥a MinMax (-1, 1)\n",
    "        4. Encode nh√£n\n",
    "        \"\"\"\n",
    "        print(f\"\\nTi·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu {dataset_name}...\")\n",
    "        print(f\"   K√≠ch th∆∞·ªõc ban ƒë·∫ßu: {X.shape}\")\n",
    "        \n",
    "        # 1. L√†m s·∫°ch d·ªØ li·ªáu\n",
    "        X_clean = X.copy()\n",
    "        X_clean = X_clean.replace([np.inf, -np.inf], np.nan)\n",
    "        X_clean = X_clean.fillna(X_clean.mean())\n",
    "\n",
    "        # 2. Lo·∫°i b·ªè c√°c feature b·∫±ng 0\n",
    "        non_zero_var_cols = X_clean.columns[(X_clean != 0).any(axis=0)]\n",
    "        X_clean = X_clean[non_zero_var_cols]\n",
    "        \n",
    "        print(f\"   Sau khi l√†m s·∫°ch: {X_clean.shape}\")\n",
    "        \n",
    "        # 3. Chu·∫©n h√≥a MinMax (-1, 1)\n",
    "        X_scaled = pd.DataFrame(\n",
    "            self.scaler.fit_transform(X_clean),\n",
    "            columns=X_clean.columns,\n",
    "            index=X_clean.index\n",
    "        )\n",
    "        \n",
    "        # 4. Encode nh√£n\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        print(f\"Ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t - shape cu·ªëi c√πng: {X_scaled.shape}\")\n",
    "        return X_scaled, y_encoded\n",
    "    \n",
    "    def apply_feature_selection(self, X_train, X_test, y_train, target_features=None):\n",
    "        \"\"\"\n",
    "        √Åp d·ª•ng 3 k·ªπ thu·∫≠t gi·∫£m chi·ªÅu d·ªØ li·ªáu:\n",
    "        1. PCA (gi·∫£m xu·ªëng ~50% s·ªë feature)\n",
    "        2. Feature Importance (ExtraTree + SelectFromModel)\n",
    "        3. Univariate Selection (SelectKBest v·ªõi chi2)\n",
    "        \"\"\"\n",
    "        print(f\"\\n√Åp d·ª•ng c√°c k·ªπ thu·∫≠t gi·∫£m chi·ªÅu d·ªØ li·ªáu...\")\n",
    "        \n",
    "        # T√≠nh s·ªë l∆∞·ª£ng features m·ª•c ti√™u (kho·∫£ng 50% theo b√†i b√°o)\n",
    "        if target_features is None:\n",
    "            target_features = max(X_train.shape[1] // 2, 10)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. PCA (gi·∫£m xu·ªëng ~50% s·ªë features)\n",
    "        print(f\"PCA: {X_train.shape[1]} ‚Üí {target_features} features\")\n",
    "        pca = PCA(n_components=target_features)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        results['PCA'] = {\n",
    "            'X_train': X_train_pca,\n",
    "            'X_test': X_test_pca,\n",
    "            'n_features': target_features,\n",
    "            'method': pca\n",
    "        }\n",
    "        \n",
    "        # 2. Feature Importance ExtraTree + SelectFromModel\n",
    "        print(f\"Feature Importance: t·ª± ƒë·ªông ch·ªçn c√°c features t·ªët nh·∫•t\")\n",
    "        extra_tree = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "        extra_tree.fit(X_train, y_train)\n",
    "        selector = SelectFromModel(extra_tree, max_features=target_features)\n",
    "        X_train_fi = selector.fit_transform(X_train, y_train)\n",
    "        X_test_fi = selector.transform(X_test)\n",
    "        results['Feature_Importance'] = {\n",
    "            'X_train': X_train_fi,\n",
    "            'X_test': X_test_fi,\n",
    "            'n_features': X_train_fi.shape[1],\n",
    "            'method': selector\n",
    "        }\n",
    "\n",
    "        # 3. Univariate Selection v·ªõi chi2 (b√†i b√°o trang 3)\n",
    "        print(f\"Univariate Selection: SelectKBest vs chi2\")\n",
    "        # Chuy·ªÉn ƒë·ªïi th√†nh gi√° tr·ªã d∆∞∆°ng cho chi2\n",
    "        X_train_pos = X_train - X_train.min() + 0.01\n",
    "        X_test_pos = X_test - X_test.min() + 0.01\n",
    "        \n",
    "        univariate = SelectKBest(score_func=chi2, k=target_features)\n",
    "        X_train_us = univariate.fit_transform(X_train_pos, y_train)\n",
    "        X_test_us = univariate.transform(X_test_pos)\n",
    "        results['Univariate_Selection'] = {\n",
    "            'X_train': X_train_us,\n",
    "            'X_test': X_test_us,\n",
    "            'n_features': target_features,\n",
    "            'method': univariate\n",
    "        }\n",
    "        \n",
    "        print(f\"Ho√†n th√†nh gi·∫£m chi·ªÅu d·ªØ li·ªáu\")\n",
    "        return results\n",
    "    \n",
    "    def evaluate_model_combination(self, X_train, X_test, y_train, y_test, \n",
    "                                 ml_name, feature_method_name, dataset_name):\n",
    "        \"\"\"\n",
    "        ƒê√°nh gi√° th·ªùi gian v√† ƒë·ªô ch√≠nh x√°c\n",
    "        \"\"\"\n",
    "        # L·∫•y m√¥ h√¨nh ML\n",
    "        model = self.ml_algorithms[ml_name]\n",
    "        \n",
    "        # Hu·∫•n luy·ªán vs ƒëo th·ªùi gian\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # D·ª± ƒëo√°n vs ƒëo th·ªùi gian\n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        predict_time = time.time() - start_time\n",
    "\n",
    "        # T√≠nh ƒë·ªô ch√≠nh x√°c\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # T·ªïng th·ªùi gian (hu·∫•n luy·ªán + d·ª± ƒëo√°n) t√≠nh b·∫±ng mili gi√¢y\n",
    "        total_time_ms = (train_time + predict_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy * 100,  # Ph·∫ßn trƒÉm\n",
    "            'total_time_ms': total_time_ms,\n",
    "            'train_time': train_time,\n",
    "            'predict_time': predict_time,\n",
    "            'y_pred': y_pred\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603cd2b",
   "metadata": {},
   "source": [
    "- T·∫°o c√°c b·ªô d·ªØ li·ªáu t·ªïng h·ª£p m√¥ ph·ªèng NSL-KDD 2019, CICIDS 2017 v√† d·ªØ li·ªáu m√¥ ph·ªèng (ƒëo·∫°n n√†y b·ªè qua v√¨ d√πng lu√¥n 2 b·ªô d·ªØ li·ªáu tr√™n kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2feb4f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·∫°o c√°c b·ªô d·ªØ li·ªáu t·ªïng h·ª£p d·ª±a tr√™n b√†i b√°o...\n",
      "\n",
      "T·∫°o b·ªô d·ªØ li·ªáu NSL-KDD 2019...\n",
      "T·∫°o b·ªô d·ªØ li·ªáu t·ªïng h·ª£p CICIDS 2017...\n",
      "T·∫°o b·ªô d·ªØ li·ªáu m√¥ ph·ªèng...\n"
     ]
    }
   ],
   "source": [
    "#b·ªè qua\n",
    "def create_synthetic_datasets():\n",
    "    \"\"\"\n",
    "    T·∫°o c√°c b·ªô d·ªØ li·ªáu t·ªïng h·ª£p m√¥ ph·ªèng ƒë·∫∑c tr∆∞ng c·ªßa 3 b·ªô d·ªØ li·ªáu trong b√†i b√°o:\n",
    "    1. NSL-KDD 2019: ~150,000 m·∫´u, 42 ƒë·∫∑c tr∆∞ng ‚Üí 21 sau gi·∫£m\n",
    "    2. CICIDS 2017: 68 ƒë·∫∑c tr∆∞ng ‚Üí 23 sau gi·∫£m  \n",
    "    3. D·ªØ li·ªáu m√¥ ph·ªèng: 45,500 m·∫´u, 73 ƒë·∫∑c tr∆∞ng ‚Üí 20 sau gi·∫£m\n",
    "    \"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    print(\"T·∫°o c√°c b·ªô d·ªØ li·ªáu t·ªïng h·ª£p d·ª±a tr√™n b√†i b√°o...\")\n",
    "    \n",
    "    # 1. Dataset simulant NSL-KDD 2019\n",
    "    print(\"\\nT·∫°o b·ªô d·ªØ li·ªáu NSL-KDD 2019...\")\n",
    "    X_nsl, y_nsl = make_classification(\n",
    "        n_samples=15000,  # s·ªë l∆∞·ª£ng m·∫´u (article: 150,000)\n",
    "        n_features=42,    \n",
    "        n_informative=35,\n",
    "        n_redundant=5,\n",
    "        n_clusters_per_class=2,\n",
    "        class_sep=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Chuy·ªÉn ƒë·ªïi sang d·ªØ li·ªáu v·ªõi t√™n c·ªôt th·ª±c t·∫ø\n",
    "    nsl_features = [f'feature_{i+1}' for i in range(42)]\n",
    "    X_nsl_df = pd.DataFrame(X_nsl, columns=nsl_features)\n",
    "    y_nsl_labels = ['Normal' if label == 0 else 'Attack' for label in y_nsl]\n",
    "    \n",
    "    datasets['NSL-KDD_2019'] = {\n",
    "        'X': X_nsl_df,\n",
    "        'y': pd.Series(y_nsl_labels),\n",
    "        'description': 'Dataset t·ªïng h·ª£p m√¥ ph·ªèng NSL-KDD 2019 (42‚Üí21 ƒë·∫∑c tr∆∞ng)'\n",
    "    }\n",
    "    \n",
    "    # 2. Dataset simulant CICIDS 2017\n",
    "    print(\"T·∫°o b·ªô d·ªØ li·ªáu t·ªïng h·ª£p CICIDS 2017...\")\n",
    "    X_cicids, y_cicids = make_classification(\n",
    "        n_samples=12000,  \n",
    "        n_features=68,    \n",
    "        n_informative=55,\n",
    "        n_redundant=8,\n",
    "        n_clusters_per_class=3,\n",
    "        class_sep=0.7,\n",
    "        random_state=123\n",
    "    )\n",
    "\n",
    "    # M√¥ ph·ªèng c√°c lo·∫°i t·∫•n c√¥ng kh√°c nhau\n",
    "    cicids_features = [f'flow_feature_{i+1}' for i in range(68)]\n",
    "    X_cicids_df = pd.DataFrame(X_cicids, columns=cicids_features)\n",
    "\n",
    "    # T·∫°o nh√£n ƒëa l·ªõp ƒë·ªÉ m√¥ ph·ªèng c√°c lo·∫°i t·∫•n c√¥ng kh√°c nhau\n",
    "    attack_types = ['Normal', 'DoS', 'DDoS', 'Brute_Force']\n",
    "    y_cicids_multiclass = np.random.choice(attack_types, size=len(y_cicids), p=[0.6, 0.2, 0.15, 0.05])\n",
    "    \n",
    "    datasets['CICIDS_2017'] = {\n",
    "        'X': X_cicids_df,\n",
    "        'y': pd.Series(y_cicids_multiclass),\n",
    "        'description': 'Dataset t·ªïng h·ª£p m√¥ ph·ªèng CICIDS 2017 (68‚Üí23 ƒë·∫∑c tr∆∞ng)'\n",
    "    }\n",
    "\n",
    "    # 3. Dataset m√¥ ph·ªèng d·ªØ li·ªáu (Lima Filho et al., 2019)\n",
    "    print(\"T·∫°o b·ªô d·ªØ li·ªáu m√¥ ph·ªèng...\")\n",
    "    X_sim, y_sim = make_classification(\n",
    "        n_samples=4550,   \n",
    "        n_features=73,   \n",
    "        n_informative=60,\n",
    "        n_redundant=10,\n",
    "        n_clusters_per_class=2,\n",
    "        class_sep=0.9,    # R·∫•t t√°ch bi·ªát ƒë·ªÉ m√¥ ph·ªèng m·ªôt m√¥i tr∆∞·ªùng ki·ªÉm so√°t\n",
    "        random_state=456\n",
    "    )\n",
    "    \n",
    "    sim_features = [f'network_metric_{i+1}' for i in range(73)]\n",
    "    X_sim_df = pd.DataFrame(X_sim, columns=sim_features)\n",
    "    y_sim_labels = ['Normal' if label == 0 else 'Attack' for label in y_sim]\n",
    "    \n",
    "    datasets['Simulated_Data'] = {\n",
    "        'X': X_sim_df,\n",
    "        'y': pd.Series(y_sim_labels),\n",
    "        'description': 'Dataset t·ªïng h·ª£p m√¥ ph·ªèng d·ªØ li·ªáu Lima Filho (73‚Üí20 ƒë·∫∑c tr∆∞ng)'\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# T·∫°o c√°c b·ªô d·ªØ li·ªáu t·ªïng h·ª£p\n",
    "synthetic_datasets = create_synthetic_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de06379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·ªï sung gi√° tr·ªã NaN v√† v√¥ c·ª±c ƒë·ªÉ ki·ªÉm tra ti·ªÅn x·ª≠ l√Ω...\n",
      "\n",
      "‚úÖ 3 datasets t·ªïng h·ª£p ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng:\n",
      "   ‚Ä¢ NSL-KDD_2019: 15000 m·∫´u, 42 ƒë·∫∑c tr∆∞ng\n",
      "     Classes: {'Normal': 7500, 'Attack': 7500}\n",
      "     NaN values: 6296\n",
      "     Inf values: 630\n",
      "   ‚Ä¢ CICIDS_2017: 12000 m·∫´u, 68 ƒë·∫∑c tr∆∞ng\n",
      "     Classes: {'Normal': 7233, 'DoS': 2376, 'DDoS': 1791, 'Brute_Force': 600}\n",
      "     NaN values: 8150\n",
      "     Inf values: 816\n",
      "   ‚Ä¢ Simulated_Data: 4550 m·∫´u, 73 ƒë·∫∑c tr∆∞ng\n",
      "     Classes: {'Attack': 2279, 'Normal': 2271}\n",
      "     NaN values: 3319\n",
      "     Inf values: 332\n"
     ]
    }
   ],
   "source": [
    "# B·ªï sung m·ªôt v√†i gi√° tr·ªã c√≥ v·∫•n ƒë·ªÅ ƒë·ªÉ ki·ªÉm tra ti·ªÅn x·ª≠ l√Ω\n",
    "print(\"B·ªï sung gi√° tr·ªã NaN v√† v√¥ c·ª±c ƒë·ªÉ ki·ªÉm tra ti·ªÅn x·ª≠ l√Ω...\")\n",
    "\n",
    "for dataset_name, dataset in synthetic_datasets.items():\n",
    "    # Ti√™m 1-2% gi√° tr·ªã c√≥ v·∫•n ƒë·ªÅ\n",
    "    n_samples, n_features = dataset['X'].shape\n",
    "    n_nan = int(0.01 * n_samples * n_features)\n",
    "\n",
    "    # V·ªã tr√≠ ng·∫´u nhi√™n cho NaN\n",
    "    nan_positions = np.random.choice(n_samples * n_features, n_nan, replace=False)\n",
    "    for pos in nan_positions:\n",
    "        row, col = divmod(pos, n_features)\n",
    "        dataset['X'].iloc[row, col] = np.nan\n",
    "\n",
    "    # M·ªôt v√†i gi√° tr·ªã v√¥ c·ª±c\n",
    "    n_inf = max(1, n_nan // 10)\n",
    "    inf_positions = np.random.choice(n_samples * n_features, n_inf, replace=False)\n",
    "    for pos in inf_positions:\n",
    "        row, col = divmod(pos, n_features)\n",
    "        dataset['X'].iloc[row, col] = np.inf if np.random.random() > 0.5 else -np.inf\n",
    "\n",
    "print(f\"\\n‚úÖ 3 datasets t·ªïng h·ª£p ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng:\")\n",
    "for name, dataset in synthetic_datasets.items():\n",
    "    print(f\"   ‚Ä¢ {name}: {dataset['X'].shape[0]} m·∫´u, {dataset['X'].shape[1]} ƒë·∫∑c tr∆∞ng\")\n",
    "    print(f\"     Classes: {dataset['y'].value_counts().to_dict()}\")\n",
    "    print(f\"     NaN values: {dataset['X'].isnull().sum().sum()}\")\n",
    "    print(f\"     Inf values: {np.isinf(dataset['X']).sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c030f",
   "metadata": {},
   "source": [
    "- Load data t·ª´ b·ªô dataset NSL-KDD 2019, CICIDS 2017 tr√™n kaggle\n",
    "- V·ªõi b·ªô NSL-KDD 2019: g√°n t√™n c·ªôt, encode m·ªôt s·ªë c·ªôt d·ªØ li·ªáu, g·ªôp 2 b·ªô d·ªØ li·ªáu train, test\n",
    "- V·ªõi CICIDS 2017: g·ªôp t·∫•t c·∫£ c√°c b·ªô d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f803f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "  \n",
    "  datasets = {}\n",
    "  # L·∫•y d·ªØ li·ªáu b·ªô NSL-KDD 2019\n",
    "  # Download latest version\n",
    "  path_nslkdd = kagglehub.dataset_download(\"hassan06/nslkdd\")\n",
    "  train_data_KDD = pd.read_csv(f\"{path_nslkdd}/KDDTrain+.txt\", header=None)\n",
    "  test_data_KDD = pd.read_csv(f\"{path_nslkdd}/KDDTest+.txt\", header=None)\n",
    "  \n",
    "  # Define the list of column names based on the NSL-KDD dataset description\n",
    "  columns = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "    'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
    "    'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',\n",
    "    'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate', 'attack', 'level'\n",
    "  ]\n",
    "\n",
    "  # Assign the column names to the dataframe\n",
    "  train_data_KDD.columns = columns\n",
    "  test_data_KDD.columns = columns\n",
    "  \n",
    "  data_KDD = pd.concat([train_data_KDD, test_data_KDD], ignore_index=True)\n",
    "\n",
    "  X_KDD = data_KDD.drop(columns=['attack', 'level'])\n",
    "  for col in X_KDD.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X_KDD[col] = le.fit_transform(X_KDD[col])\n",
    "  y_KDD = data_KDD['attack']\n",
    "\n",
    "  datasets['NSL-KDD'] = {\n",
    "        'X': X_KDD,\n",
    "        'y': y_KDD,\n",
    "        'description': 'NSL-KDD dataset t·ª´ Kaggle'\n",
    "    }\n",
    "\n",
    "  # L·∫•y d·ªØ li·ªáu b·ªô CICIDS 2017\n",
    "  # Download latest version\n",
    "  path_cicids = kagglehub.dataset_download(\"chethuhn/network-intrusion-dataset\")\n",
    "\n",
    "  # Get all CSV files in the directory\n",
    "  csv_files = glob.glob(os.path.join(path_cicids, \"*.csv\"))\n",
    "  # Read and concatenate all CSV files into a single DataFrame\n",
    "  df_2017 = pd.concat((pd.read_csv(file) for file in csv_files), ignore_index=True)\n",
    "  df_2017.columns = df_2017.columns.str.strip()\n",
    "  #print(df_2017['Label'].value_counts())\n",
    "  df_2017['Label'] = df_2017['Label'].str.strip()\n",
    "\n",
    "  X_cicids = df_2017.drop(columns=['Label'])\n",
    "  y_cicids = df_2017['Label']\n",
    "  datasets['CICIDS_2017'] = {\n",
    "        'X': X_cicids,\n",
    "        'y': y_cicids,\n",
    "        'description': 'CICIDS 2017 dataset t·ª´ Kaggle'\n",
    "    }\n",
    "  return datasets\n",
    "datasets = load_dataset()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92816d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NSL-KDD': {'X':         duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0              0              1       20     9        491          0     0   \n",
      "1              0              2       44     9        146          0     0   \n",
      "2              0              1       49     5          0          0     0   \n",
      "3              0              1       24     9        232       8153     0   \n",
      "4              0              1       24     9        199        420     0   \n",
      "...          ...            ...      ...   ...        ...        ...   ...   \n",
      "148512         0              1       54     9        794        333     0   \n",
      "148513         0              1       24     9        317        938     0   \n",
      "148514         0              1       24     9      54540       8314     0   \n",
      "148515         0              2       12     9         42         42     0   \n",
      "148516         0              1       57     1          0          0     0   \n",
      "\n",
      "        wrong_fragment  urgent  hot  ...  dst_host_count  dst_host_srv_count  \\\n",
      "0                    0       0    0  ...             150                  25   \n",
      "1                    0       0    0  ...             255                   1   \n",
      "2                    0       0    0  ...             255                  26   \n",
      "3                    0       0    0  ...              30                 255   \n",
      "4                    0       0    0  ...             255                 255   \n",
      "...                ...     ...  ...  ...             ...                 ...   \n",
      "148512               0       0    0  ...             100                 141   \n",
      "148513               0       0    0  ...             197                 255   \n",
      "148514               0       0    2  ...             255                 255   \n",
      "148515               0       0    0  ...             255                 252   \n",
      "148516               0       0    0  ...             255                  21   \n",
      "\n",
      "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                         0.17                    0.03   \n",
      "1                         0.00                    0.60   \n",
      "2                         0.10                    0.05   \n",
      "3                         1.00                    0.00   \n",
      "4                         1.00                    0.00   \n",
      "...                        ...                     ...   \n",
      "148512                    0.72                    0.06   \n",
      "148513                    1.00                    0.00   \n",
      "148514                    1.00                    0.00   \n",
      "148515                    0.99                    0.01   \n",
      "148516                    0.08                    0.03   \n",
      "\n",
      "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                              0.17                         0.00   \n",
      "1                              0.88                         0.00   \n",
      "2                              0.00                         0.00   \n",
      "3                              0.03                         0.04   \n",
      "4                              0.00                         0.00   \n",
      "...                             ...                          ...   \n",
      "148512                         0.01                         0.01   \n",
      "148513                         0.01                         0.01   \n",
      "148514                         0.00                         0.00   \n",
      "148515                         0.00                         0.00   \n",
      "148516                         0.00                         0.00   \n",
      "\n",
      "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                       0.00                      0.00                  0.05   \n",
      "1                       0.00                      0.00                  0.00   \n",
      "2                       1.00                      1.00                  0.00   \n",
      "3                       0.03                      0.01                  0.00   \n",
      "4                       0.00                      0.00                  0.00   \n",
      "...                      ...                       ...                   ...   \n",
      "148512                  0.01                      0.00                  0.00   \n",
      "148513                  0.01                      0.00                  0.00   \n",
      "148514                  0.00                      0.00                  0.07   \n",
      "148515                  0.00                      0.00                  0.00   \n",
      "148516                  0.00                      0.00                  0.44   \n",
      "\n",
      "        dst_host_srv_rerror_rate  \n",
      "0                           0.00  \n",
      "1                           0.00  \n",
      "2                           0.00  \n",
      "3                           0.01  \n",
      "4                           0.00  \n",
      "...                          ...  \n",
      "148512                      0.00  \n",
      "148513                      0.00  \n",
      "148514                      0.07  \n",
      "148515                      0.00  \n",
      "148516                      1.00  \n",
      "\n",
      "[148517 rows x 41 columns], 'y': 0          normal\n",
      "1          normal\n",
      "2         neptune\n",
      "3          normal\n",
      "4          normal\n",
      "           ...   \n",
      "148512     normal\n",
      "148513     normal\n",
      "148514       back\n",
      "148515     normal\n",
      "148516      mscan\n",
      "Name: attack, Length: 148517, dtype: object, 'description': 'NSL-KDD dataset t·ª´ Kaggle'}, 'CICIDS_2017': {'X':          Destination Port  Flow Duration  Total Fwd Packets  \\\n",
      "0                   54865              3                  2   \n",
      "1                   55054            109                  1   \n",
      "2                   55055             52                  1   \n",
      "3                   46236             34                  1   \n",
      "4                   54863              3                  2   \n",
      "...                   ...            ...                ...   \n",
      "2830738                53          32215                  4   \n",
      "2830739                53            324                  2   \n",
      "2830740             58030             82                  2   \n",
      "2830741                53        1048635                  6   \n",
      "2830742                53          94939                  4   \n",
      "\n",
      "         Total Backward Packets  Total Length of Fwd Packets  \\\n",
      "0                             0                           12   \n",
      "1                             1                            6   \n",
      "2                             1                            6   \n",
      "3                             1                            6   \n",
      "4                             0                           12   \n",
      "...                         ...                          ...   \n",
      "2830738                       2                          112   \n",
      "2830739                       2                           84   \n",
      "2830740                       1                           31   \n",
      "2830741                       2                          192   \n",
      "2830742                       2                          188   \n",
      "\n",
      "         Total Length of Bwd Packets  Fwd Packet Length Max  \\\n",
      "0                                  0                      6   \n",
      "1                                  6                      6   \n",
      "2                                  6                      6   \n",
      "3                                  6                      6   \n",
      "4                                  0                      6   \n",
      "...                              ...                    ...   \n",
      "2830738                          152                     28   \n",
      "2830739                          362                     42   \n",
      "2830740                            6                     31   \n",
      "2830741                          256                     32   \n",
      "2830742                          226                     47   \n",
      "\n",
      "         Fwd Packet Length Min  Fwd Packet Length Mean  Fwd Packet Length Std  \\\n",
      "0                            6                     6.0                0.00000   \n",
      "1                            6                     6.0                0.00000   \n",
      "2                            6                     6.0                0.00000   \n",
      "3                            6                     6.0                0.00000   \n",
      "4                            6                     6.0                0.00000   \n",
      "...                        ...                     ...                    ...   \n",
      "2830738                     28                    28.0                0.00000   \n",
      "2830739                     42                    42.0                0.00000   \n",
      "2830740                      0                    15.5               21.92031   \n",
      "2830741                     32                    32.0                0.00000   \n",
      "2830742                     47                    47.0                0.00000   \n",
      "\n",
      "         ...  act_data_pkt_fwd  min_seg_size_forward  Active Mean  Active Std  \\\n",
      "0        ...                 1                    20          0.0         0.0   \n",
      "1        ...                 0                    20          0.0         0.0   \n",
      "2        ...                 0                    20          0.0         0.0   \n",
      "3        ...                 0                    20          0.0         0.0   \n",
      "4        ...                 1                    20          0.0         0.0   \n",
      "...      ...               ...                   ...          ...         ...   \n",
      "2830738  ...                 3                    20          0.0         0.0   \n",
      "2830739  ...                 1                    20          0.0         0.0   \n",
      "2830740  ...                 0                    32          0.0         0.0   \n",
      "2830741  ...                 5                    20          0.0         0.0   \n",
      "2830742  ...                 3                    20          0.0         0.0   \n",
      "\n",
      "         Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min  \n",
      "0                 0           0        0.0       0.0         0         0  \n",
      "1                 0           0        0.0       0.0         0         0  \n",
      "2                 0           0        0.0       0.0         0         0  \n",
      "3                 0           0        0.0       0.0         0         0  \n",
      "4                 0           0        0.0       0.0         0         0  \n",
      "...             ...         ...        ...       ...       ...       ...  \n",
      "2830738           0           0        0.0       0.0         0         0  \n",
      "2830739           0           0        0.0       0.0         0         0  \n",
      "2830740           0           0        0.0       0.0         0         0  \n",
      "2830741           0           0        0.0       0.0         0         0  \n",
      "2830742           0           0        0.0       0.0         0         0  \n",
      "\n",
      "[2830743 rows x 78 columns], 'y': 0          BENIGN\n",
      "1          BENIGN\n",
      "2          BENIGN\n",
      "3          BENIGN\n",
      "4          BENIGN\n",
      "            ...  \n",
      "2830738    BENIGN\n",
      "2830739    BENIGN\n",
      "2830740    BENIGN\n",
      "2830741    BENIGN\n",
      "2830742    BENIGN\n",
      "Name: Label, Length: 2830743, dtype: object, 'description': 'CICIDS 2017 dataset t·ª´ Kaggle'}}\n"
     ]
    }
   ],
   "source": [
    "print((datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10785da5",
   "metadata": {},
   "source": [
    "Implement systematic evaluation with time/accuracy measurement \n",
    "- Tri·ªÉn khai ƒë√°nh gi√° h·ªá th·ªëng v√° ƒëo th·ªùi gian/ƒë·ªô ch√≠nh x√°c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4639915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·∫°o c√°c b·ªô d·ªØ li·ªáu t·ªïng h·ª£p d·ª±a tr√™n b√†i b√°o...\n",
      "\n",
      "T·∫°o b·ªô d·ªØ li·ªáu NSL-KDD 2019...\n",
      "T·∫°o b·ªô d·ªØ li·ªáu t·ªïng h·ª£p CICIDS 2017...\n",
      "T·∫°o b·ªô d·ªØ li·ªáu m√¥ ph·ªèng...\n",
      "B·∫Øt ƒë·∫ßu ƒë√°nh gi√°\n",
      "================================================================================\n",
      "\n",
      "ƒê√°nh gi√° b·ªô d·ªØ li·ªáu: NSL-KDD_2019\n",
      "   Dataset t·ªïng h·ª£p m√¥ ph·ªèng NSL-KDD 2019 (42‚Üí21 ƒë·∫∑c tr∆∞ng)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu NSL-KDD_2019...\n",
      "   K√≠ch th∆∞·ªõc ban ƒë·∫ßu: (15000, 42)\n",
      "   Sau khi l√†m s·∫°ch: (15000, 42)\n",
      "Ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t - shape cu·ªëi c√πng: (15000, 42)\n",
      "\n",
      "   Division:\n",
      "      Train: 12000 m·∫´u\n",
      "      Test: 3000 m·∫´u\n",
      "\n",
      "√Åp d·ª•ng c√°c k·ªπ thu·∫≠t gi·∫£m chi·ªÅu d·ªØ li·ªáu...\n",
      "PCA: 42 ‚Üí 21 features\n",
      "Feature Importance: t·ª± ƒë·ªông ch·ªçn c√°c features t·ªët nh·∫•t\n",
      "Univariate Selection: SelectKBest vs chi2\n",
      "Ho√†n th√†nh gi·∫£m chi·ªÅu d·ªØ li·ªáu\n",
      "\n",
      "   ƒê√ÅNH GI√Å C√ÅC K·∫æT H·ª¢P ML + Feature Selection:\n",
      "   Thu·∫≠t to√°n      Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu ƒê·ªô ch√≠nh x√°c (%) Th·ªùi gian (ms) S·ªë features\n",
      "   --------------- ------------------ ------------ ---------- --------\n",
      "\n",
      "   BASELINE (kh√¥ng gi·∫£m chi·ªÅu):\n",
      "   KNN             No_Reduction       97.00        335.4      42      \n",
      "   AdaBoost        No_Reduction       79.43        2600.1     42      \n",
      "   RandomForest    No_Reduction       93.60        4682.3     42      \n",
      "   SVM             No_Reduction       98.57        1098.3     42      \n",
      "\n",
      "   SAU KHI GI·∫¢M CHI·ªÄU:\n",
      "\n",
      "   üîπ PCA:\n",
      "     KNN             PCA                89.40        33.0       21      \n",
      "     AdaBoost        PCA                77.53        1365.5     21      \n",
      "     RandomForest    PCA                87.47        3285.2     21      \n",
      "     SVM             PCA                93.03        1396.5     21      \n",
      "\n",
      "   üîπ Feature_Importance:\n",
      "     KNN             Feature_Importance 86.93        486.0      15      \n",
      "     AdaBoost        Feature_Importance 76.63        965.7      15      \n",
      "     RandomForest    Feature_Importance 86.17        2419.6     15      \n",
      "     SVM             Feature_Importance 90.40        1450.7     15      \n",
      "\n",
      "   üîπ Univariate_Selection:\n",
      "     KNN             Univariate_Selection 88.80        79.5       21      \n",
      "     AdaBoost        Univariate_Selection 75.27        1335.4     21      \n",
      "     RandomForest    Univariate_Selection 86.80        3134.9     21      \n",
      "     SVM             Univariate_Selection 90.70        1265.9     21      \n",
      "\n",
      "   S·ª± k·∫øt h·ª£p t·ªët nh·∫•t NSL-KDD_2019:\n",
      "      SVM + No_Reduction\n",
      "      ƒê·ªô ch√≠nh x√°c: 98.57%\n",
      "      Th·ªùi gian: 1098.3 ms\n",
      "      S·ªë ƒë·∫∑c ƒëi·ªÉm: 42\n",
      "\n",
      "ƒê√°nh gi√° b·ªô d·ªØ li·ªáu: CICIDS_2017\n",
      "   Dataset t·ªïng h·ª£p m√¥ ph·ªèng CICIDS 2017 (68‚Üí23 ƒë·∫∑c tr∆∞ng)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu CICIDS_2017...\n",
      "   K√≠ch th∆∞·ªõc ban ƒë·∫ßu: (12000, 68)\n",
      "   Sau khi l√†m s·∫°ch: (12000, 68)\n",
      "Ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t - shape cu·ªëi c√πng: (12000, 68)\n",
      "\n",
      "   Division:\n",
      "      Train: 9600 m·∫´u\n",
      "      Test: 2400 m·∫´u\n",
      "\n",
      "√Åp d·ª•ng c√°c k·ªπ thu·∫≠t gi·∫£m chi·ªÅu d·ªØ li·ªáu...\n",
      "PCA: 68 ‚Üí 34 features\n",
      "Feature Importance: t·ª± ƒë·ªông ch·ªçn c√°c features t·ªët nh·∫•t\n",
      "Univariate Selection: SelectKBest vs chi2\n",
      "Ho√†n th√†nh gi·∫£m chi·ªÅu d·ªØ li·ªáu\n",
      "\n",
      "   ƒê√ÅNH GI√Å C√ÅC K·∫æT H·ª¢P ML + Feature Selection:\n",
      "   Thu·∫≠t to√°n      Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu ƒê·ªô ch√≠nh x√°c (%) Th·ªùi gian (ms) S·ªë features\n",
      "   --------------- ------------------ ------------ ---------- --------\n",
      "\n",
      "   BASELINE (kh√¥ng gi·∫£m chi·ªÅu):\n",
      "   KNN             No_Reduction       45.79        102.7      68      \n",
      "   AdaBoost        No_Reduction       59.83        3311.2     68      \n",
      "   RandomForest    No_Reduction       60.04        8503.8     68      \n",
      "   SVM             No_Reduction       60.04        5615.5     68      \n",
      "\n",
      "   SAU KHI GI·∫¢M CHI·ªÄU:\n",
      "\n",
      "   üîπ PCA:\n",
      "     KNN             PCA                46.04        15.6       34      \n",
      "     AdaBoost        PCA                59.79        1750.4     34      \n",
      "     RandomForest    PCA                60.04        5485.4     34      \n",
      "     SVM             PCA                60.04        4497.9     34      \n",
      "\n",
      "   üîπ Feature_Importance:\n",
      "     KNN             Feature_Importance 46.46        69.7       34      \n",
      "     AdaBoost        Feature_Importance 60.00        1719.4     34      \n",
      "     RandomForest    Feature_Importance 60.00        5241.1     34      \n",
      "     SVM             Feature_Importance 60.04        4330.2     34      \n",
      "\n",
      "   üîπ Univariate_Selection:\n",
      "     KNN             Univariate_Selection 46.04        90.4       34      \n",
      "     AdaBoost        Univariate_Selection 59.33        1718.9     34      \n",
      "     RandomForest    Univariate_Selection 59.92        5216.3     34      \n",
      "     SVM             Univariate_Selection 60.04        4317.7     34      \n",
      "\n",
      "   S·ª± k·∫øt h·ª£p t·ªët nh·∫•t CICIDS_2017:\n",
      "      RandomForest + No_Reduction\n",
      "      ƒê·ªô ch√≠nh x√°c: 60.04%\n",
      "      Th·ªùi gian: 8503.8 ms\n",
      "      S·ªë ƒë·∫∑c ƒëi·ªÉm: 68\n",
      "\n",
      "ƒê√°nh gi√° b·ªô d·ªØ li·ªáu: Simulated_Data\n",
      "   Dataset t·ªïng h·ª£p m√¥ ph·ªèng d·ªØ li·ªáu Lima Filho (73‚Üí20 ƒë·∫∑c tr∆∞ng)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu Simulated_Data...\n",
      "   K√≠ch th∆∞·ªõc ban ƒë·∫ßu: (4550, 73)\n",
      "   Sau khi l√†m s·∫°ch: (4550, 73)\n",
      "Ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t - shape cu·ªëi c√πng: (4550, 73)\n",
      "\n",
      "   Division:\n",
      "      Train: 3640 m·∫´u\n",
      "      Test: 910 m·∫´u\n",
      "\n",
      "√Åp d·ª•ng c√°c k·ªπ thu·∫≠t gi·∫£m chi·ªÅu d·ªØ li·ªáu...\n",
      "PCA: 73 ‚Üí 36 features\n",
      "Feature Importance: t·ª± ƒë·ªông ch·ªçn c√°c features t·ªët nh·∫•t\n",
      "Univariate Selection: SelectKBest vs chi2\n",
      "Ho√†n th√†nh gi·∫£m chi·ªÅu d·ªØ li·ªáu\n",
      "\n",
      "   ƒê√ÅNH GI√Å C√ÅC K·∫æT H·ª¢P ML + Feature Selection:\n",
      "   Thu·∫≠t to√°n      Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu ƒê·ªô ch√≠nh x√°c (%) Th·ªùi gian (ms) S·ªë features\n",
      "   --------------- ------------------ ------------ ---------- --------\n",
      "\n",
      "   BASELINE (kh√¥ng gi·∫£m chi·ªÅu):\n",
      "   KNN             No_Reduction       97.03        50.8       73      \n",
      "   AdaBoost        No_Reduction       74.95        1297.7     73      \n",
      "   RandomForest    No_Reduction       91.32        1598.6     73      \n",
      "   SVM             No_Reduction       98.57        284.2      73      \n",
      "\n",
      "   SAU KHI GI·∫¢M CHI·ªÄU:\n",
      "\n",
      "   üîπ PCA:\n",
      "     KNN             PCA                88.46        31.3       36      \n",
      "     AdaBoost        PCA                74.95        665.9      36      \n",
      "     RandomForest    PCA                86.92        1271.7     36      \n",
      "     SVM             PCA                95.82        201.0      36      \n",
      "\n",
      "   üîπ Feature_Importance:\n",
      "     KNN             Feature_Importance 87.69        50.2       29      \n",
      "     AdaBoost        Feature_Importance 74.73        550.3      29      \n",
      "     RandomForest    Feature_Importance 84.07        1016.0     29      \n",
      "     SVM             Feature_Importance 91.87        185.5      29      \n",
      "\n",
      "   üîπ Univariate_Selection:\n",
      "     KNN             Univariate_Selection 87.58        49.6       36      \n",
      "     AdaBoost        Univariate_Selection 71.32        661.5      36      \n",
      "     RandomForest    Univariate_Selection 74.95        1203.2     36      \n",
      "     SVM             Univariate_Selection 91.54        188.6      36      \n",
      "\n",
      "   S·ª± k·∫øt h·ª£p t·ªët nh·∫•t Simulated_Data:\n",
      "      SVM + No_Reduction\n",
      "      ƒê·ªô ch√≠nh x√°c: 98.57%\n",
      "      Th·ªùi gian: 284.2 ms\n",
      "      S·ªë ƒë·∫∑c ƒëi·ªÉm: 73\n"
     ]
    }
   ],
   "source": [
    "def run_complete_evaluation(classifier, datasets):\n",
    "    \"\"\"\n",
    "    Th·ª±c hi·ªán ƒë√°nh gi√° ƒë·∫ßy ƒë·ªß theo ph∆∞∆°ng ph√°p c·ªßa b√†i vi·∫øt:\n",
    "    - Ki·ªÉm tra c√°c thu·∫≠t to√°n 4 ml v·ªõi 3 k·ªπ thu·∫≠t l·ª±a ch·ªçn t√≠nh nƒÉng\n",
    "    - ƒêo l∆∞·ªùng th·ªùi gian th·ª±c hi·ªán v√† ƒë·ªô ch√≠nh x√°c\n",
    "    - T·∫°o b·∫£ng k·∫øt qu·∫£\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"B·∫Øt ƒë·∫ßu ƒë√°nh gi√°\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "\n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        print(f\"\\nƒê√°nh gi√° b·ªô d·ªØ li·ªáu: {dataset_name}\")\n",
    "        print(f\"   {dataset_info['description']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        X, y = dataset_info['X'], dataset_info['y']\n",
    "        \n",
    "        # 1. Ti·ªÅn x·ª≠ l√Ω\n",
    "        X_processed, y_processed = classifier.preprocess_data(X, y, dataset_name)\n",
    "\n",
    "        # 2. Division train/test (80/20)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Division:\")\n",
    "        print(f\"      Train: {X_train.shape[0]} m·∫´u\")\n",
    "        print(f\"      Test: {X_test.shape[0]} m·∫´u\")\n",
    "        \n",
    "        # 3. √Åp d·ª•ng c√°c k·ªπ thu·∫≠t l·ª±a gi·∫£m chi·ªÅu\n",
    "        feature_selection_results = classifier.apply_feature_selection(\n",
    "            X_train, X_test, y_train\n",
    "        )\n",
    "        \n",
    "        # 4. ƒê√°nh gi√° k·∫øt h·ª£p ML + Feature Selection\n",
    "        dataset_results = {}\n",
    "        \n",
    "        print(f\"\\n   ƒê√ÅNH GI√Å C√ÅC K·∫æT H·ª¢P ML + Feature Selection:\")\n",
    "        print(f\"   {'Thu·∫≠t to√°n':<15} {'Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu':<18} {'ƒê·ªô ch√≠nh x√°c (%)':<12} {'Th·ªùi gian (ms)':<10} {'S·ªë features':<8}\")\n",
    "        print(f\"   {'-'*15} {'-'*18} {'-'*12} {'-'*10} {'-'*8}\")\n",
    "        \n",
    "        # ƒê√°nh gi√° tr∆∞·ªõc khi gi·∫£m chi·ªÅu (baseline)\n",
    "        print(f\"\\n   BASELINE (kh√¥ng gi·∫£m chi·ªÅu):\")\n",
    "        for ml_name in classifier.ml_algorithms.keys():\n",
    "            result = classifier.evaluate_model_combination(\n",
    "                X_train, X_test, y_train, y_test, ml_name, \"No_Reduction\", dataset_name\n",
    "            )\n",
    "            \n",
    "            dataset_results[f\"{ml_name}_No_Reduction\"] = {\n",
    "                'ml_algorithm': ml_name,\n",
    "                'feature_method': 'No_Reduction',\n",
    "                'accuracy': result['accuracy'],\n",
    "                'time_ms': result['total_time_ms'],\n",
    "                'n_features': X_train.shape[1]\n",
    "            }\n",
    "            \n",
    "            print(f\"   {ml_name:<15} {'No_Reduction':<18} {result['accuracy']:<12.2f} {result['total_time_ms']:<10.1f} {X_train.shape[1]:<8}\")\n",
    "        \n",
    "        # ƒë√°nh gi√° khi gi·∫£m chi·ªÅu\n",
    "        print(f\"\\n   SAU KHI GI·∫¢M CHI·ªÄU:\")\n",
    "        for feature_method, feature_data in feature_selection_results.items():\n",
    "            print(f\"\\n   üîπ {feature_method}:\")\n",
    "            \n",
    "            X_train_reduced = feature_data['X_train']\n",
    "            X_test_reduced = feature_data['X_test']\n",
    "            n_features = feature_data['n_features']\n",
    "            \n",
    "            for ml_name in classifier.ml_algorithms.keys():\n",
    "                result = classifier.evaluate_model_combination(\n",
    "                    X_train_reduced, X_test_reduced, y_train, y_test, \n",
    "                    ml_name, feature_method, dataset_name\n",
    "                )\n",
    "                \n",
    "                key = f\"{ml_name}_{feature_method}\"\n",
    "                dataset_results[key] = {\n",
    "                    'ml_algorithm': ml_name,\n",
    "                    'feature_method': feature_method,\n",
    "                    'accuracy': result['accuracy'],\n",
    "                    'time_ms': result['total_time_ms'],\n",
    "                    'n_features': n_features\n",
    "                }\n",
    "                \n",
    "                print(f\"     {ml_name:<15} {feature_method:<18} {result['accuracy']:<12.2f} {result['total_time_ms']:<10.1f} {n_features:<8}\")\n",
    "        \n",
    "        all_results[dataset_name] = dataset_results\n",
    "        \n",
    "        # X√°c ƒë·ªãnh s·ª± k·∫øt h·ª£p t·ªët nh·∫•t cho b·ªô d·ªØ li·ªáu n√†y\n",
    "        best_combo = max(dataset_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "        print(f\"\\n   S·ª± k·∫øt h·ª£p t·ªët nh·∫•t {dataset_name}:\")\n",
    "        print(f\"      {best_combo[1]['ml_algorithm']} + {best_combo[1]['feature_method']}\")\n",
    "        print(f\"      ƒê·ªô ch√≠nh x√°c: {best_combo[1]['accuracy']:.2f}%\")\n",
    "        print(f\"      Th·ªùi gian: {best_combo[1]['time_ms']:.1f} ms\")\n",
    "        print(f\"      S·ªë ƒë·∫∑c ƒëi·ªÉm: {best_combo[1]['n_features']}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Kh·ªüi t·∫°o tr√¨nh ph√¢n lo·∫°i v√† th·ª±c hi·ªán ƒë√°nh gi√°\n",
    "classifier = DoSDDoSClassifier()\n",
    "#evaluation_results = run_complete_evaluation(classifier, load_dataset())\n",
    "evaluation_results = run_complete_evaluation(classifier, create_synthetic_datasets())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8075a",
   "metadata": {},
   "source": [
    "- T·∫°o b·∫£ng k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2910a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·∫†O C√ÅC B·∫¢NG K·∫æT QU·∫¢\n",
      "================================================================================\n",
      "\n",
      "B·∫¢NG K·∫æT QU·∫¢ - NSL-KDD_2019\n",
      "----------------------------------------------------------------------\n",
      "Thu·∫≠t to√°n ML Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu ƒê·ªô ch√≠nh x√°c (%) Th·ªùi gian (ms)  S·ªë features\n",
      "          SVM           No_Reduction            98.57         1098.3           42\n",
      "          KNN           No_Reduction            97.00          335.4           42\n",
      " RandomForest           No_Reduction            93.60         4682.3           42\n",
      "          SVM                    PCA            93.03         1396.5           21\n",
      "          SVM   Univariate_Selection            90.70         1265.9           21\n",
      "          SVM     Feature_Importance            90.40         1450.7           15\n",
      "          KNN                    PCA            89.40           33.0           21\n",
      "          KNN   Univariate_Selection            88.80           79.5           21\n",
      " RandomForest                    PCA            87.47         3285.2           21\n",
      "          KNN     Feature_Importance            86.93          486.0           15\n",
      " RandomForest   Univariate_Selection            86.80         3134.9           21\n",
      " RandomForest     Feature_Importance            86.17         2419.6           15\n",
      "     AdaBoost           No_Reduction            79.43         2600.1           42\n",
      "     AdaBoost                    PCA            77.53         1365.5           21\n",
      "     AdaBoost     Feature_Importance            76.63          965.7           15\n",
      "     AdaBoost   Univariate_Selection            75.27         1335.4           21\n",
      "\n",
      "  K·∫æT QU·∫¢ T·ªêT NH·∫§T:\n",
      "      SVM + No_Reduction\n",
      "      ƒê·ªô ch√≠nh x√°c: 98.57%\n",
      "      Th·ªùi gian: 1098.3 ms\n",
      "      S·ªë features: 42\n",
      "\n",
      "  PH√ÇN T√çCH C·∫¢I THI·ªÜN:\n",
      "      Gi·∫£m th·ªùi gian trung b√¨nh: 34.2%\n",
      "      Th·ªùi gian trung b√¨nh baseline: 2179.0 ms\n",
      "      Th·ªùi gian trung b√¨nh v·ªõi gi·∫£m chi·ªÅu: 1434.8 ms\n",
      "\n",
      "B·∫¢NG K·∫æT QU·∫¢ - CICIDS_2017\n",
      "----------------------------------------------------------------------\n",
      "Thu·∫≠t to√°n ML Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu ƒê·ªô ch√≠nh x√°c (%) Th·ªùi gian (ms)  S·ªë features\n",
      " RandomForest           No_Reduction            60.04         8503.8           68\n",
      "          SVM           No_Reduction            60.04         5615.5           68\n",
      " RandomForest                    PCA            60.04         5485.4           34\n",
      "          SVM                    PCA            60.04         4497.9           34\n",
      "          SVM     Feature_Importance            60.04         4330.2           34\n",
      "          SVM   Univariate_Selection            60.04         4317.7           34\n",
      "     AdaBoost     Feature_Importance            60.00         1719.4           34\n",
      " RandomForest     Feature_Importance            60.00         5241.1           34\n",
      " RandomForest   Univariate_Selection            59.92         5216.3           34\n",
      "     AdaBoost           No_Reduction            59.83         3311.2           68\n",
      "     AdaBoost                    PCA            59.79         1750.4           34\n",
      "     AdaBoost   Univariate_Selection            59.33         1718.9           34\n",
      "          KNN     Feature_Importance            46.46           69.7           34\n",
      "          KNN                    PCA            46.04           15.6           34\n",
      "          KNN   Univariate_Selection            46.04           90.4           34\n",
      "          KNN           No_Reduction            45.79          102.7           68\n",
      "\n",
      "  K·∫æT QU·∫¢ T·ªêT NH·∫§T:\n",
      "      RandomForest + No_Reduction\n",
      "      ƒê·ªô ch√≠nh x√°c: 60.04%\n",
      "      Th·ªùi gian: 8503.8 ms\n",
      "      S·ªë features: 68\n",
      "\n",
      "  PH√ÇN T√çCH C·∫¢I THI·ªÜN:\n",
      "      Gi·∫£m th·ªùi gian trung b√¨nh: 34.5%\n",
      "      Th·ªùi gian trung b√¨nh baseline: 4383.3 ms\n",
      "      Th·ªùi gian trung b√¨nh v·ªõi gi·∫£m chi·ªÅu: 2871.1 ms\n",
      "\n",
      "B·∫¢NG K·∫æT QU·∫¢ - Simulated_Data\n",
      "----------------------------------------------------------------------\n",
      "Thu·∫≠t to√°n ML Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu ƒê·ªô ch√≠nh x√°c (%) Th·ªùi gian (ms)  S·ªë features\n",
      "          SVM           No_Reduction            98.57          284.2           73\n",
      "          KNN           No_Reduction            97.03           50.8           73\n",
      "          SVM                    PCA            95.82          201.0           36\n",
      "          SVM     Feature_Importance            91.87          185.5           29\n",
      "          SVM   Univariate_Selection            91.54          188.6           36\n",
      " RandomForest           No_Reduction            91.32         1598.6           73\n",
      "          KNN                    PCA            88.46           31.3           36\n",
      "          KNN     Feature_Importance            87.69           50.2           29\n",
      "          KNN   Univariate_Selection            87.58           49.6           36\n",
      " RandomForest                    PCA            86.92         1271.7           36\n",
      " RandomForest     Feature_Importance            84.07         1016.0           29\n",
      "     AdaBoost           No_Reduction            74.95         1297.7           73\n",
      "     AdaBoost                    PCA            74.95          665.9           36\n",
      " RandomForest   Univariate_Selection            74.95         1203.2           36\n",
      "     AdaBoost     Feature_Importance            74.73          550.3           29\n",
      "     AdaBoost   Univariate_Selection            71.32          661.5           36\n",
      "\n",
      "  K·∫æT QU·∫¢ T·ªêT NH·∫§T:\n",
      "      SVM + No_Reduction\n",
      "      ƒê·ªô ch√≠nh x√°c: 98.57%\n",
      "      Th·ªùi gian: 284.2 ms\n",
      "      S·ªë features: 73\n",
      "\n",
      "  PH√ÇN T√çCH C·∫¢I THI·ªÜN:\n",
      "      Gi·∫£m th·ªùi gian trung b√¨nh: 37.3%\n",
      "      Th·ªùi gian trung b√¨nh baseline: 807.8 ms\n",
      "      Th·ªùi gian trung b√¨nh v·ªõi gi·∫£m chi·ªÅu: 506.2 ms\n"
     ]
    }
   ],
   "source": [
    "def create_results_tables(evaluation_results):\n",
    "    \"\"\"\n",
    "    t·∫°o b·∫£ng k·∫øt qu·∫£\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"T·∫†O C√ÅC B·∫¢NG K·∫æT QU·∫¢\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for dataset_name, results in evaluation_results.items():\n",
    "        print(f\"\\nB·∫¢NG K·∫æT QU·∫¢ - {dataset_name}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # t·∫°o 1 DataFrame ƒë·ªÉ hi·ªÉn th·ªã b·∫£ng\n",
    "        table_data = []\n",
    "        \n",
    "        for combo_name, combo_results in results.items():\n",
    "            ml_algo = combo_results['ml_algorithm']\n",
    "            feature_method = combo_results['feature_method']\n",
    "            accuracy = combo_results['accuracy']\n",
    "            time_ms = combo_results['time_ms']\n",
    "            n_features = combo_results['n_features']\n",
    "            \n",
    "            table_data.append({\n",
    "                'Thu·∫≠t to√°n ML': ml_algo,\n",
    "                'Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu': feature_method,\n",
    "                'ƒê·ªô ch√≠nh x√°c (%)': f\"{accuracy:.2f}\",\n",
    "                'Th·ªùi gian (ms)': f\"{time_ms:.1f}\",\n",
    "                'S·ªë features': n_features\n",
    "            })\n",
    "        \n",
    "        df_results = pd.DataFrame(table_data)\n",
    "        \n",
    "        # S·∫Øp x·∫øp b·∫±ng c√°ch gi·∫£m ƒë·ªô ch√≠nh x√°c\n",
    "        df_results = df_results.sort_values('ƒê·ªô ch√≠nh x√°c (%)', ascending=False)\n",
    "\n",
    "        print(df_results.to_string(index=False))\n",
    "\n",
    "        # Th·ªëng k√™ b·ªï sung\n",
    "        best_accuracy = df_results.iloc[0]\n",
    "        print(f\"\\n  K·∫æT QU·∫¢ T·ªêT NH·∫§T:\")\n",
    "        print(f\"      {best_accuracy['Thu·∫≠t to√°n ML']} + {best_accuracy['Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu']}\")\n",
    "        print(f\"      ƒê·ªô ch√≠nh x√°c: {best_accuracy['ƒê·ªô ch√≠nh x√°c (%)']}%\")\n",
    "        print(f\"      Th·ªùi gian: {best_accuracy['Th·ªùi gian (ms)']} ms\")\n",
    "        print(f\"      S·ªë features: {best_accuracy['S·ªë features']}\")\n",
    "\n",
    "        # Ph√¢n t√≠ch c·∫£i thi·ªán th·ªùi gian so v·ªõi ƒë·ªô ch√≠nh x√°c\n",
    "        baseline_results = df_results[df_results['Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu'] == 'No_Reduction']\n",
    "        reduced_results = df_results[df_results['Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu'] != 'No_Reduction']\n",
    "\n",
    "        if not baseline_results.empty and not reduced_results.empty:\n",
    "            avg_baseline_time = baseline_results['Th·ªùi gian (ms)'].str.replace(' ms', '').astype(float).mean()\n",
    "            avg_reduced_time = reduced_results['Th·ªùi gian (ms)'].str.replace(' ms', '').astype(float).mean()\n",
    "\n",
    "            time_improvement = ((avg_baseline_time - avg_reduced_time) / avg_baseline_time) * 100\n",
    "\n",
    "            print(f\"\\n  PH√ÇN T√çCH C·∫¢I THI·ªÜN:\")\n",
    "            print(f\"      Gi·∫£m th·ªùi gian trung b√¨nh: {time_improvement:.1f}%\")\n",
    "            print(f\"      Th·ªùi gian trung b√¨nh baseline: {avg_baseline_time:.1f} ms\")\n",
    "            print(f\"      Th·ªùi gian trung b√¨nh v·ªõi gi·∫£m chi·ªÅu: {avg_reduced_time:.1f} ms\")\n",
    "\n",
    "# T·∫°o b·∫£ng k·∫øt qu·∫£\n",
    "create_results_tables(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c4ecea",
   "metadata": {},
   "source": [
    "Create performance visualizations and comparisons\n",
    "- T·∫°o bi·ªÉu ƒë·ªì so s√°nh hi·ªáu nƒÉng"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
